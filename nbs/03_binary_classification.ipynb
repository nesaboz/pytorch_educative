{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pytorched.step_by_step import StepByStep, RUNS_FOLDER_NAME\n",
    "import platform\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Scikit-Learnâ€™s make_moons to generate a toy dataset with 1000 data points and two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=11)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use Scikit-Learn's `StandardScaler` to standardize datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)  # always fit only on X_train\n",
    "X_train_scalled = sc.transform(X_train)\n",
    "X_val_scalled = sc.transform(X_val)  # DO NOT use fit or fit_transform on X_val, it causes data leak\n",
    "m = sc.mean_\n",
    "v = sc.var_\n",
    "print(m, v)\n",
    "assert ((X_train_scalled[0] - m)/np.sqrt(v) - X_train[0] < np.finfo(float).eps).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_scalled\n",
    "X_val = X_val_scalled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def figure1(X_train, y_train, X_val, y_val, cm_bright=None):\n",
    "    if cm_bright is None:\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)#, edgecolors='k')\n",
    "    ax[0].set_xlabel(r'$X_1$')\n",
    "    ax[0].set_ylabel(r'$X_2$')\n",
    "    ax[0].set_xlim([-2.3, 2.3])\n",
    "    ax[0].set_ylim([-2.3, 2.3])\n",
    "    ax[0].set_title('Generated Data - Train')\n",
    "\n",
    "    ax[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright)#, edgecolors='k')\n",
    "    ax[1].set_xlabel(r'$X_1$')\n",
    "    ax[1].set_ylabel(r'$X_2$')\n",
    "    ax[1].set_xlim([-2.3, 2.3])\n",
    "    ax[1].set_ylim([-2.3, 2.3])\n",
    "    ax[1].set_title('Generated Data - Validation')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure1(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation of data starts by **converting the data points** from Numpy arrays to PyTorch tensors and sending them to the available **device**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Builds tensors from numpy arrays\n",
    "x_train_tensor = torch.as_tensor(X_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()  # reshape makes shape from (80,) to (80,1)\n",
    "\n",
    "x_val_tensor = torch.as_tensor(X_val).float()\n",
    "y_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, 64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear', nn.Linear(2,1))\n",
    "model.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_lin = StepByStep(model, optimizer, loss_fn)\n",
    "sbs_lin.set_loaders(train_loader, val_loader)\n",
    "sbs_lin.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sbs_lin.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the values for `X_train` (`y_train_predicted`) and `X_val` (`y_val_predicted`) and plot them. Also let's see how good of a job linear regression did using confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_plot_count(sbs):\n",
    "    y_train_predicted = sbs.predict(X_train)\n",
    "    y_val_predicted = sbs.predict(X_val)\n",
    "    fig = figure1(X_train, y_train_predicted, X_val, y_val_predicted)\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_val, list(map(int, (y_val_predicted > 0.5).ravel()))))\n",
    "    print('Correct categories:')\n",
    "    print(sbs.loader_apply(sbs.val_loader, sbs.correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot_count(sbs_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see there are some false positives and false negatives (off-diagonal elements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nonlin = nn.Sequential()\n",
    "model_nonlin.add_module('linear1', nn.Linear(2,10))\n",
    "model_nonlin.add_module('relu', nn.ReLU())\n",
    "model_nonlin.add_module('linear2', nn.Linear(10,1))\n",
    "model_nonlin.add_module('sigmoid', nn.Sigmoid())\n",
    "optimizer = optim.Adam(model_nonlin.parameters(), lr=lr)\n",
    "\n",
    "sbs_nonlin = StepByStep(model_nonlin, optimizer, loss_fn)\n",
    "sbs_nonlin.set_loaders(train_loader, val_loader)\n",
    "sbs_nonlin.train(100)\n",
    "_ = sbs_nonlin.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot_count(sbs_nonlin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is better (we could calculate precision and recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nonlin2 = nn.Sequential(\n",
    "    nn.Linear(2,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,1),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "optimizer = optim.Adam(model_nonlin2.parameters(), lr=lr)\n",
    "\n",
    "sbs_nonlin2 = StepByStep(model_nonlin2, optimizer, loss_fn)\n",
    "sbs_nonlin2.set_loaders(train_loader, val_loader)\n",
    "sbs_nonlin2.train(200)\n",
    "_ = sbs_nonlin2.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot_count(sbs_nonlin2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much better then 2-layer model, and we can see that model might be overfitting considering flattness of `val_loss`. But overall not bad considering the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying SGD instead of Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nonlin2 = nn.Sequential(\n",
    "    nn.Linear(2,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,1),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "optimizer = optim.SGD(model_nonlin2.parameters(), lr=lr)\n",
    "\n",
    "sbs_nonlin3 = StepByStep(model_nonlin2, optimizer, loss_fn)\n",
    "sbs_nonlin3.set_loaders(train_loader, val_loader)\n",
    "sbs_nonlin3.train(200)\n",
    "_ = sbs_nonlin3.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot_count(sbs_nonlin3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Adam optimizer is performing much better then SGD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorched",
   "language": "python",
   "name": "pytorched"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

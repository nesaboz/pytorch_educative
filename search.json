[
  {
    "objectID": "bear_classifier.html",
    "href": "bear_classifier.html",
    "title": "Bear classifier",
    "section": "",
    "text": "See a deployed app here.\nThere will be two parts to it:\n- first part generates a model\n- second part deploys it as an app on HuggingFace (see code)"
  },
  {
    "objectID": "bear_classifier.html#from-data-to-dataloaders",
    "href": "bear_classifier.html#from-data-to-dataloaders",
    "title": "Bear classifier",
    "section": "From Data to DataLoaders",
    "text": "From Data to DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = bears.dataloaders(path)\n\n\ndls.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "bear_classifier.html#training-your-model-and-using-it-to-clean-your-data",
    "href": "bear_classifier.html#training-your-model-and-using-it-to-clean-your-data",
    "title": "Bear classifier",
    "section": "Training Your Model, and Using It to Clean Your Data",
    "text": "Training Your Model, and Using It to Clean Your Data\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\n\n\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.027743\n      0.179141\n      0.062500\n      01:22\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.114028\n      0.170333\n      0.026786\n      01:35\n    \n    \n      1\n      0.096802\n      0.224060\n      0.026786\n      01:28\n    \n    \n      2\n      0.081567\n      0.242517\n      0.026786\n      01:22\n    \n    \n      3\n      0.065939\n      0.244289\n      0.026786\n      01:20\n    \n  \n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=2, figsize=(10, 10))\n\n\n\n\n\n\n\n\n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n#hide\nfor idx in cleaner.delete(): \n    cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): \n    shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "bear_classifier.html#turning-your-model-into-an-online-application",
    "href": "bear_classifier.html#turning-your-model-into-an-online-application",
    "title": "Bear classifier",
    "section": "Turning Your Model into an Online Application",
    "text": "Turning Your Model into an Online Application\n\nUsing the Model for Inference\n\nlearn.export('bears.pkl')\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('bears.pkl')]\n\n\n\nlearn_inf = load_learner(path/'bears.pkl')\n\n\nlearn_inf.predict('bear_images/grizzly.jpg')\n\n\n\n\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([2.1257e-06, 1.0000e+00, 2.7545e-08]))\n\n\n\nlearn_inf.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\n\n\nCreating a Notebook App from the Model\n\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n\n\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nbtn_upload = SimpleNamespace(data = ['bear_images/grizzly.jpg'])\n\n\nimg = PILImage.create(btn_upload.data[-1])\n\n\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\n\n\n\npred,pred_idx,probs = learn_inf.predict(img)\n\n\n\n\n\n\n\n\n\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n\n\n\n\n\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\n\n\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\n\n#hide\n#Putting back btn_upload to a widget for next cell\nbtn_upload = widgets.FileUpload()\n\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])\n\n\n\n\nAll together:\n\npath = Path()\nlearn_inf = load_learner(path/'bears.pkl')\n\nout_pl = widgets.Output()\nbtn_upload = widgets.FileUpload()\nlbl_pred = widgets.Label()\nbtn_run = widgets.Button(description='Classify')\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nbtn_run.on_click(on_click_classify)\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Summary",
    "section": "",
    "text": "This repo covers my journey in variety of existing Machine Learning projects. Please follow the links in the sidebar.\nIn addition, you can also see my own projects:\n- Solar panel detection project\nAlso useful: my cheat-sheet\nReferences:\n- PyTorch: Step by Step books (3 of them) by Daniel Voigt Godoy\n- Excellent course by Yann LeCun & Alfredo Canziani\n- Practical Deep Learning for Coders course by Jeremy Howard."
  },
  {
    "objectID": "labeling.html",
    "href": "labeling.html",
    "title": "Labeling",
    "section": "",
    "text": "from PIL import Image, ImageDraw\nfrom pathlib import Path\nimport json"
  },
  {
    "objectID": "labeling.html#labelme",
    "href": "labeling.html#labelme",
    "title": "Labeling",
    "section": "LabelMe",
    "text": "LabelMe\nLabelMe is simple-to-use GUI for labeling. It is straightforward to use (pip install labelme, run in terminal as labelme):\n\n\n\nlabelme\n\n\nThe segmentation points will be saved as json files:\n{\n  \"version\": \"5.1.1\",  \n  \"flags\": {},  \n  \"shapes\": [\n    {\n      \"label\": \"1\",\n      \"points\": [\n        [\n          4.029411764705877,\n          1.0882352941176439\n        ],\n        [\n          0.7941176470588189,\n          53.73529411764706\n        ],\n        [\n          0.7941176470588189,\n          221.97058823529412\n        ],\n        [\n          12.852941176470587,\n          1.3823529411764672\n        ]\n      ],\n      \"group_id\": null,\n      \"shape_type\": \"polygon\",\n      \"flags\": {}\n    },\n    ...\nLet’s load some image and generated json file:\n\nim = Image.open(\"images/solar_panels.png\")\nwidth, height = im.size\nprint(width, height)\nim\n\n256 256\n\n\n\n\n\nLet’s convert the json into a mask:\n\nwith open('images/solar_panels.json') as f:\n    data = json.load(f)\n\nThere are:\n\npoints = data['shapes']\nlen(points)\n\n20\n\n\n20 data points, let’s make a mask out of those:\n\nmask = Image.new('L', (width, height), 0)\nfor group in points:\n    label_class = group['label']\n    polygon = group['points']\n    polygon = [(round(x), round(y)) for x,y in polygon]\n    ImageDraw.Draw(mask).polygon(polygon, outline=255, fill=255)\nmask\n\n\n\n\nand save this as a file:\n\nmask.save('images/solar_panels_mask.png')\n\n\nassert Path('images/solar_panels_mask.png').is_file()"
  },
  {
    "objectID": "multiclass_image_classification.html",
    "href": "multiclass_image_classification.html",
    "title": "Multiclass image classification",
    "section": "",
    "text": "Here we’ll modify binary image classification from previous example to multiclass image classification by detecting left diagonal and right diagonal separately.\n\n\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom pytorched.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\n\n\n\n\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\n\n\nData\n\n\nCode\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start > 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets > 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30, per_row=10):\n    n_rows = n_plot // per_row + ((n_plot % per_row) > 0)\n    fig, axes = plt.subplots(n_rows, per_row, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // per_row, i % per_row    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 8})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n\n\nimages, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=13)\n\n\nfig = plot_images(images, labels, n_plot=30)\n\n\n\n\n\n\nData preparation\nWe prepare data similary as in the previous exercize thought notable difference is that y_tensor shape is (N), not (N,1) as previously. This is due to loss function (CrossEntropyLoss) requiremnts (it takes class indices).\n\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels).long()\n\n\nx_tensor.shape\n\ntorch.Size([1000, 1, 10, 10])\n\n\n\ny_tensor.shape\n\ntorch.Size([1000])\n\n\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\n\n\ntorch.manual_seed(42)\nN = len(x_tensor)\nn_train = int(.8*N)\ntrain_subset, val_subset = random_split(x_tensor, [n_train, N - n_train])\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\nWe do not apply augmentation since it would mess up the labels:\n\ntrain_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\nval_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n\nNow we can build train/val tensors, Datasets and DataLoaders:\n\nx_train_tensor = x_tensor[train_idx]\ny_train_tensor = y_tensor[train_idx]\n\nx_val_tensor = x_tensor[val_idx]\ny_val_tensor = y_tensor[val_idx]\n\ntrain_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\nval_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\n\ny_val_tensor\n\ntensor([2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n        2, 1, 2, 2, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 0, 0, 0, 2,\n        0, 2, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1, 0,\n        2, 0, 0, 0, 1, 0, 2, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2,\n        2, 2, 0, 2, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 2, 0, 2, 2, 2, 1, 1,\n        1, 1, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1,\n        0, 1, 0, 2, 0, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 2,\n        2, 1, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0,\n        0, 0, 0, 2, 2, 2, 2, 2])\n\n\n\n\nDeep model\nA typical architecture uses a sequence of one or more typical convolutional blocks, with each block consisting of three operations:\n\nConvolution\nActivation function\nPooling\n\nAnd for multiclass problems we need to use appropriate loss functions depending if we have Sigmoid/LogSoftmax as the last layer:\n\nLet’s build a model:\n\ntorch.manual_seed(13)\nmodel_cnn1 = nn.Sequential()\n\n# Featurizer\n# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\nn_channels = 1\nmodel_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\nmodel_cnn1.add_module('relu1', nn.ReLU())\nmodel_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n# Flattening: n_channels * 4 * 4\nmodel_cnn1.add_module('flatten', nn.Flatten())\n\n# Classification\n# Hidden Layer\nmodel_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\nmodel_cnn1.add_module('relu2', nn.ReLU())\n# Output Layer\nmodel_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))\n\nlr = 0.1\nmulti_loss_fn = nn.CrossEntropyLoss(reduction='mean')\noptimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)\n\nLet’s just run one batch to get the sense for loss:\n\nx, y = next(iter(train_loader))\ny_pred = model_cnn1(x)\nprint(y.shape)\nprint(y_pred.shape)\nnn.CrossEntropyLoss()(y_pred,y)\n\ntorch.Size([16])\ntorch.Size([16, 3])\n\n\ntensor(0.2768, grad_fn=<NllLossBackward0>)\n\n\nOne important observation: y shape is 16, while y_pred shape is 16x3. CrossEntropyLoss expects this (see docs).\n\nsbs = StepByStep(model_cnn1, optimizer_cnn1, multi_loss_fn)\n# sbs.set_seed()\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(20)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  7.16it/s]\n\n\n\nfig = sbs.plot_losses()\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs.loader_apply(sbs.val_loader, sbs.correct))\n\n\nCorrect categories:\ntensor([[60, 71],\n        [46, 56],\n        [73, 73]])\n\n\n\n\nCode\nprint(f'Accuracy: {sbs.accuracy}%')\n\n\nAccuracy: 89.5%\n\n\nThis is not the greatest accuracy for label 0 (parallel) and label 1 (counter-diagonal). Once can probably find a better model (TODO) but for now let’s inspect what failed.\n\n\nVisualize error outputs\n\n# will predict all points at once here, no batches:\nlogits = sbs.predict(val_loader.dataset.x)\npredicted = np.argmax(logits, 1)\n\n\nlogits.shape\n\n(200, 3)\n\n\n\nval_loader.dataset.x.shape\n\ntorch.Size([200, 1, 10, 10])\n\n\n\nlogits.shape\n\n(200, 3)\n\n\n\npredicted\n\narray([2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n       0, 1, 2, 1, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1,\n       0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0,\n       1, 2, 1, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 0, 2, 1,\n       2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1,\n       1, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2,\n       0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 2, 0,\n       2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 1, 1,\n       0, 2, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2,\n       2, 2])\n\n\n\nnot_equal = torch.ne(val_loader.dataset.y, torch.as_tensor(predicted))\nimages_tensor = val_loader.dataset.x[not_equal]\nactual_labels_tensor = val_loader.dataset.y[not_equal]\npred_labels_tensor = predicted[not_equal]\n\n\nfeaturizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\nclassifier_layers = ['fc1', 'relu2', 'fc2']\n\nsbs.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n\nstart_idx = 0\nbatch_size = 10\nimages_batch = images_tensor[start_idx:start_idx+batch_size]\nlabels_batch = actual_labels_tensor[start_idx:start_idx+batch_size]\n\nlogits = sbs.predict(images_batch)\npredicted = np.argmax(logits, 1)\nsbs.remove_hooks()\n\n\nwith plt.style.context('seaborn-white'):\n    fig_maps1 = sbs.visualize_outputs(featurizer_layers)\n    fig_maps2 = sbs.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)\n\n\n\n\n\n\n\nAnd we see that lots of predictions are for class label 2. For images 2,3,7,8, filters failed to register anything.\n\n\nOrdinary batch visualization:\n\nfig_filters = sbs.visualize_filters('conv1', cmap='gray')\n\n\n\n\n\nfeaturizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\nclassifier_layers = ['fc1', 'relu2', 'fc2']\n\nsbs.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n\nimages_batch, labels_batch = next(iter(val_loader))\nlogits = sbs.predict(images_batch)\npredicted = np.argmax(logits, 1)\nsbs.remove_hooks()\n\n\nwith plt.style.context('seaborn-white'):\n    fig_maps1 = sbs.visualize_outputs(featurizer_layers)\n    fig_maps2 = sbs.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
  },
  {
    "objectID": "quadratic_function_fit.html",
    "href": "quadratic_function_fit.html",
    "title": "Quadratic function fit",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\nfrom pytorched.step_by_step import StepByStep\n\nfrom torchviz import make_dot\nplt.style.use('fivethirtyeight')\nLet’s make up some data:"
  },
  {
    "objectID": "quadratic_function_fit.html#same-but-using-sklearn.pipeline",
    "href": "quadratic_function_fit.html#same-but-using-sklearn.pipeline",
    "title": "Quadratic function fit",
    "section": "Same but using sklearn.pipeline",
    "text": "Same but using sklearn.pipeline\nOne can make the process more streamlined using Pipeline:\n\nmodel = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                  ('linear', LinearRegression(fit_intercept=False))])\n# fit to an order-2 polynomial data\nmodel = model.fit(x, y)\nprint(model.named_steps['linear'].coef_)\nprint(f'Real values {a0}, {a1}, {a2}')\n\n[[ 4.98812164 -1.61954639  4.02342307]]\nReal values 5.0, -1.5, 4.0"
  },
  {
    "objectID": "quadratic_function_fit.html#data-preparation",
    "href": "quadratic_function_fit.html#data-preparation",
    "title": "Quadratic function fit",
    "section": "Data Preparation",
    "text": "Data Preparation\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\nnp.random.seed(42)\nN = len(x)\nidx = list(range(N))\nnp.random.shuffle(idx)\n\n\nsplit_idx = int(.8*N)\ntrain_idx = idx[:split_idx]\nval_idx = idx[split_idx:]\ntrain_x = torch.as_tensor(x[train_idx], device=device).float()\ntrain_y = torch.as_tensor(y[train_idx], device=device).float()\nval_x = torch.as_tensor(x[val_idx], device=device).float()\nval_y = torch.as_tensor(y[val_idx], device=device).float()\n\n\ntrain_dataset = TensorDataset(train_x, train_y)\nval_dataset = TensorDataset(val_x, val_y)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)"
  },
  {
    "objectID": "quadratic_function_fit.html#training",
    "href": "quadratic_function_fit.html#training",
    "title": "Quadratic function fit",
    "section": "Training",
    "text": "Training\n\nmodel=nn.Sequential(\n        nn.Linear(1,10),\n        nn.ReLU(),\n        nn.Linear(10,1)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nloss_fn = nn.MSELoss()\n\nsbs = StepByStep(model, optimizer, loss_fn)\n\nLet’s train for 200 epoch and plot losses:\n\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(200)\n\n\nsbs.plot_losses()\n\n\n\n\nLet’s make predictions:\n\ntest =np.linspace(-4.,4.,num=N).reshape(-1,1)\ntest_predictions = sbs.predict(test)\nplt.plot(x,y,'.')\nplt.plot(test,test_predictions,'.')\nplt.show()\n\n\n\n\nThis is good though, unfortunatelly, the true values of quadratic function are now lost in the sea of weights of the the two linear layers:\n\nsbs.model.state_dict()\n\nOrderedDict([('0.weight',\n              tensor([[ 1.3475],\n                      [-2.2383],\n                      [-2.1243],\n                      [ 2.0004],\n                      [-1.9875],\n                      [-2.2052],\n                      [ 0.1436],\n                      [-1.8479],\n                      [ 2.6974],\n                      [ 2.1781]])),\n             ('0.bias',\n              tensor([-1.2300, -3.2117,  0.8249, -1.5303, -0.2013, -2.3025,  1.3949, -0.0182,\n                       0.2817, -3.1922])),\n             ('2.weight',\n              tensor([[0.7446, 2.5052, 1.1556, 1.2103, 1.3438, 1.6768, 0.8039, 1.2448, 1.4132,\n                       2.6946]])),\n             ('2.bias', tensor([1.5188]))])"
  },
  {
    "objectID": "2_torchvision_conversions_samplers.html",
    "href": "2_torchvision_conversions_samplers.html",
    "title": "Torchvision, Conversions, Samplers",
    "section": "",
    "text": "Here we’ll take a problem of binary image classifing."
  },
  {
    "objectID": "2_torchvision_conversions_samplers.html#note-on-nchw-vs-nhwc-formats",
    "href": "2_torchvision_conversions_samplers.html#note-on-nchw-vs-nhwc-formats",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Note on NCHW vs NHWC formats",
    "text": "Note on NCHW vs NHWC formats\nThe format for images is different between libraries: - PyTorch uses NCHW - TensorFlow uses NHWC - PIL/Matplotlib images are HWC.\n\nimage_r  = np.zeros((5, 5), dtype=np.uint8)\nimage_r[:, 0] = 255\nimage_r[:, 1] = 128\n\nimage_g = np.zeros((5, 5), dtype=np.uint8)\nimage_g[:, 1] = 128\nimage_g[:, 2] = 255\nimage_g[:, 3] = 128\n\nimage_b = np.zeros((5, 5), dtype=np.uint8)\nimage_b[:, 3] = 128\nimage_b[:, 4] = 255\n\n\n\nCode\nshow_image(image_r, 'gray')\nzeros = np.zeros((5,5), dtype=np.uint8)\nstacked_red = np.zeros((5,5,3), dtype=np.uint8)  # the format is HxWxX (height X width X channel)\nstacked_red[:,:,0] = image_r  \n# also same thing can be achieved stacked_red = np.stack([image_r, zeros, zeros], axis=2)\nshow_image(stacked_red)\n\n\n\n\nCode\ndef image_channels(red, green, blue, rgb, gray, rows=(0, 1, 2)):\n    fig, axs = plt.subplots(len(rows), 4, figsize=(15, 5.5))\n\n    zeros = np.zeros((5, 5), dtype=np.uint8)\n\n    titles1 = ['Red', 'Green', 'Blue', 'Grayscale Image']\n    titles0 = ['image_r', 'image_g', 'image_b', 'image_gray']\n    titles2 = ['as first channel', 'as second channel', 'as third channel', 'RGB Image']\n\n    idx0 = np.argmax(np.array(rows) == 0)\n    idx1 = np.argmax(np.array(rows) == 1)\n    idx2 = np.argmax(np.array(rows) == 2)\n    \n    for i, m in enumerate([red, green, blue, gray]):\n        if 0 in rows:\n            axs[idx0, i].axis('off')\n            axs[idx0, i].invert_yaxis()\n            if (1 in rows) or (i < 3):\n                axs[idx0, i].text(0.15, 0.25, str(m.astype(np.uint8)), verticalalignment='top')    \n                axs[idx0, i].set_title(titles0[i], fontsize=16)\n\n        if 1 in rows:\n            axs[idx1, i].set_title(titles1[i], fontsize=16)\n            axs[idx1, i].set_xlabel('5x5', fontsize=14)\n            axs[idx1, i].imshow(m, cmap=plt.cm.gray)\n\n        if 2 in rows:\n            axs[idx2, i].set_title(titles2[i], fontsize=16)\n            axs[idx2, i].set_xlabel(f'5x5x3 - {titles1[i][0]} only', fontsize=14)\n            if i < 3:\n                stacked = [zeros] * 3\n                stacked[i] = m\n                axs[idx2, i].imshow(np.stack(stacked, axis=2))\n            else:\n                axs[idx2, i].imshow(rgb)\n\n        for r in [1, 2]:\n            if r in rows:\n                idx = idx1 if r == 1 else idx2\n                axs[idx, i].set_xticks([])\n                axs[idx, i].set_yticks([])\n                for k, v in axs[idx, i].spines.items():\n                    v.set_color('black')\n                    v.set_linewidth(.8)\n\n    if 1 in rows:\n        axs[idx1, 0].set_ylabel('Single\\nChannel\\n(grayscale)', rotation=0, labelpad=40, fontsize=12)\n        axs[idx1, 3].set_xlabel('5x5 = 0.21R + 0.72G + 0.07B')\n    if 2 in rows:\n        axs[idx2, 0].set_ylabel('Three\\nChannels\\n(color)', rotation=0, labelpad=40, fontsize=12)\n        axs[idx2, 3].set_xlabel('5x5x3 = (R, G, B) stacked')\n    fig.tight_layout()\n    return fig\n\n\n\nimage_gray = .2126*image_r + .7152*image_g + .0722*image_b\nimage_rgb = np.stack([image_r, image_g, image_b], axis=2)\nfig = image_channels(image_r, image_g, image_b, image_rgb, image_gray, rows=(0, 1))\n\n\n\n\n\nfig = image_channels(image_r, image_g, image_b, image_rgb, image_gray, rows=(0, 2))"
  },
  {
    "objectID": "2_torchvision_conversions_samplers.html#note-on-torchvision",
    "href": "2_torchvision_conversions_samplers.html#note-on-torchvision",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Note on Torchvision",
    "text": "Note on Torchvision\ntorchvision has many existing Datasets, Models, and Transformations.\n\nConversion between ndarray, PIL, and tensor\n\nimages.shape\n\n(300, 1, 5, 5)\n\n\n\nexample_chw = images[3]\nexample_chw\n\narray([[[  0,   0, 255,   0,   0],\n        [  0, 255,   0,   0,   0],\n        [255,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0]]], dtype=uint8)\n\n\n\nexample_chw.shape\n\n(1, 5, 5)\n\n\n\nexample_hwc = np.transpose(example_chw, (1,2,0))\nexample_hwc.shape\n\n(5, 5, 1)\n\n\n\nshow_image(example_hwc, 'gray')\n\n\n\n\nToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\ntorch.as_tensor can work on N-dimensional arrays (ToTensor works only on 2D/3D images), but doesn’t apply scalling and, just fyi, it shares data i.e. doesn’t make a copy.\n\ntensorizer = ToTensor()\nexample_tensor = tensorizer(example_hwc)\nexample_tensor.shape\n\ntorch.Size([1, 5, 5])\n\n\n\nexample_tensor\n\ntensor([[[0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]])\n\n\n\ntype(example_tensor)\n\ntorch.Tensor\n\n\n\nexample_pil = ToPILImage()(example_tensor)  # similar to np.transpose(example_tensor, (1,2,0))\nprint(type(example_pil))\n\n<class 'PIL.Image.Image'>\n\n\n\nshow_image(example_pil, 'gray')\n\n\n\n\nTo convert between Tensor and Numpy:\n\nexample_np = example_tensor.detach().cpu().numpy()\nprint(type(example_np))\n\n<class 'numpy.ndarray'>\n\n\nTo convert between numpy and PIL:\n\nexample_np = np.array(example_pil)\nprint(type(example_np))\n\n<class 'numpy.ndarray'>\n\n\n\nexample_pil_back = Image.fromarray(example_np)\nprint(type(example_pil_back))\n\n<class 'PIL.Image.Image'>\n\n\nmore options for different PIL modes: (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1).\nOne has to be careful: - PIL gray images (L mode) need to be squeezed from 3D to 2D via np.squeeze() - RGB mode images should be floats between [0,1] - L mode (grayscale) images should be np.uint8 from [0,255] - 1 mode (bool) images are True or False.\n\n# PIL_image = Image.fromarray(np.uint8(example_np)).convert('RGB')\n# PIL_image = Image.fromarray(example_np.astype('uint8'), 'RGB')\n\n\nshow_image(example_pil_back, 'gray')\n\n\n\n\n\n\nTransformations\nThere are many transformations that can be done on tensors and/or PIL images:\n\nflipper = RandomHorizontalFlip(p=1.0)\nshow_image(flipper(example_pil), 'gray')\n\n\n\n\n\nprint(example_tensor)\nnormalizer = Normalize(mean=.5, std=.5)\nnormalized_tensor = normalizer(example_tensor)\nprint(normalized_tensor)\n\ntensor([[[0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]])\ntensor([[[-1., -1.,  1., -1., -1.],\n         [-1.,  1., -1., -1., -1.],\n         [ 1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.]]])\n\n\n\n\nComposing transforms\n\ncomposer = Compose([RandomHorizontalFlip(p=1.0),\n                    Normalize(mean=(.5,), std=(.5,))])\ncomposer(example_tensor)\n\ntensor([[[-1., -1.,  1., -1., -1.],\n         [-1., -1., -1.,  1., -1.],\n         [-1., -1., -1., -1.,  1.],\n         [-1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.]]])\n\n\n\ntorch.as_tensor(example_np/255).float()\n\ntensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nIf we want to convert the whole numpy array to tensor we can use:\n\nexample_tensor = torch.as_tensor(example_np / 255).float()\nexample_tensor\n\ntensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nPyTorch default floating point dtype is torch.float32, one can change this if needed via torch.set_default_dtype(torch.float64)."
  },
  {
    "objectID": "2_torchvision_conversions_samplers.html#samplers",
    "href": "2_torchvision_conversions_samplers.html#samplers",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Samplers",
    "text": "Samplers\nOne can also define a Sampler (and it’s subclasses: SequentialSampler, RandomSampler, SubsetRandomSampler, WeightedRandomSampler, BatchSampler, and DistributedSampler), use WeightedRandomSampler for example in case data is unbalanced.\n\ncomposer = Compose([RandomHorizontalFlip(p=0.5),\n                    Normalize(mean=.5, std=.5)])\n\ndataset = TransformedTensorDataset(x_tensor, y_tensor, transform=composer)\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=dataset, batch_size=16, sampler=train_sampler)\nval_loader = DataLoader(dataset=dataset, batch_size=16, sampler=val_sampler)\n\nNote that we need val_sampler just because we are passing the full dataset.\nAlso note that when using Samplers, one can’t use shuffle=True in DataLoaders.\n\nlen(iter(train_loader)), len(iter(val_loader))\n\n(15, 4)"
  },
  {
    "objectID": "2_torchvision_conversions_samplers.html#some-model-methods",
    "href": "2_torchvision_conversions_samplers.html#some-model-methods",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Some Model methods",
    "text": "Some Model methods\nTo see the layers:\n\nsbs_deeper.model\n\nSequential(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear1): Linear(in_features=25, out_features=10, bias=True)\n  (relu): ReLU()\n  (linear2): Linear(in_features=10, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)\n\n\n\nlist(sbs_deeper.model.named_modules())\n\n[('',\n  Sequential(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (linear1): Linear(in_features=25, out_features=10, bias=True)\n    (relu): ReLU()\n    (linear2): Linear(in_features=10, out_features=1, bias=True)\n    (sigmoid): Sigmoid()\n  )),\n ('flatten', Flatten(start_dim=1, end_dim=-1)),\n ('linear1', Linear(in_features=25, out_features=10, bias=True)),\n ('relu', ReLU()),\n ('linear2', Linear(in_features=10, out_features=1, bias=True)),\n ('sigmoid', Sigmoid())]\n\n\nTo see particular weights in a layer:\n\nsbs_deeper.model.linear2.weight.detach()\n\ntensor([[ 0.1024,  0.2038,  0.2085, -0.2298,  0.1973, -0.2550,  0.2233, -0.0626,\n         -0.0030,  0.0018]])\n\n\n\nsbs_deeper.count_parameters()\n\n271\n\n\nThe 271 comes from: (25 * 10 + 10 biases) + (10 * 1 + 1 bias) = 260 + 11 = 271\nto reset parameters I implemented this way but this doesn’t reset all parameters (for example in nn.PReLU) so use caution!\n\nsbs_deeper.reset_parameters()"
  },
  {
    "objectID": "transfer_learning.html",
    "href": "transfer_learning.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Set the environment (for Google Colab):\nIf this is GoogleColab we download config and use it to download necesseary folders and files for this project (defined in config.py). Here we’ll need: - pytorched.step_by_step.py - data_preparation.rps.py\n\n\nCode\ntry:\n    import google.colab\n    !pip install numpy pandas matplotlib torchviz scikit-learn tensorboard torchvision torch tqdm torch-lr-finder\n\n    import requests\n    url = 'https://raw.githubusercontent.com/nesaboz/pytorched/main/config.py'\n    r = requests.get(url, allow_redirects=True)\n    open('config.py', 'wb').write(r.content)    \nexcept ModuleNotFoundError:\n    print('Not Google Colab environment.')\n\n\nfrom config import config_project\nconfig_project('transfer_learning')\n\n\nWe are now ready for imports:\n\n\nCode\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\nfrom torchvision.transforms import Compose, ToTensor, Normalize, Resize, ToPILImage, CenterCrop, RandomResizedCrop, InterpolationMode\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import alexnet, resnet18, inception_v3\nfrom torchvision.models.alexnet import model_urls\nfrom torchvision.models import Inception_V3_Weights, AlexNet_Weights\nfrom torch.hub import load_state_dict_from_url\nfrom torchviz import make_dot\n\nfrom data_generation.rps import download_rps\nfrom pytorched.step_by_step import StepByStep, freeze_model, print_trainable_parameters\n\nplt.style.use('fivethirtyeight')\n\n\n\nAlexNet\nLet’s try to use AlexNet model first to help us in the Rock-Paper-Scissors problem. We’ll need to load AlexNet, with it’s weights, then make a feature-extractor on our data loaders, modify the last layer, and train. Let’s get to it:\nWe can get AlexNet from torchvision.models:\n\nweights = AlexNet_Weights.IMAGENET1K_V1\nalex = alexnet(weights=weights)\nalex\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\nWe need an original transform:\n\ntransform = weights.transforms()\n\nLet’s now create new data loaders based on this transform:\n\n\nCode\ndownload_rps()\n\n\n\ntrain_dataset = ImageFolder(root='rps', transform=transform)\nval_dataset = ImageFolder(root='rps-test-set', transform=transform)\n\n\ntrain_dataset.classes\n\n['paper', 'rock', 'scissors']\n\n\n\ntrain_loader = DataLoader(train_dataset, 16, shuffle=True)\nval_loader = DataLoader(val_dataset, 16)\n\nLet’s also define an optimizer and loss:\n\ntorch.manual_seed(17)\noptimizer = optim.Adam(alex.parameters(), 3e-4)\nloss_fn = nn.CrossEntropyLoss()\nsbs = StepByStep(alex, optimizer, loss_fn)\nsbs.set_loaders(train_loader, val_loader)\n\n\n\nFeature-extractor\nThese are current trainable parameters:\n\nsbs.print_trainable_parameters()\n\nfeatures.0.weight\nfeatures.0.bias\nfeatures.3.weight\nfeatures.3.bias\nfeatures.6.weight\nfeatures.6.bias\nfeatures.8.weight\nfeatures.8.bias\nfeatures.10.weight\nfeatures.10.bias\nclassifier.1.weight\nclassifier.1.bias\nclassifier.4.weight\nclassifier.4.bias\nclassifier.6.weight\nclassifier.6.bias\n\n\nWe must freeze the model and replace the last layer. These are suggestions what layers to change (of course more layers can be left unfrozen but this requires more training data and longer times):\n\nso let’s change the last layer first to Identity to make feature extractor:\n\nsbs.model.classifier[6] = nn.Identity()\n\n\nfreeze_model(sbs.model)\n\n\nsbs.print_trainable_parameters()\n\nNo trainable parameters.\n\n\nWe now go throught the loader batch by batch and pass the data through the model in order to generate new preprocessed datasets:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef preprocessed_dataset(model, loader, device=None):\n    \"\"\"\n    Runs all data in the loader through the model and returns a dataset.\n    \"\"\"\n    \n    features = torch.Tensor()\n    labels = torch.Tensor().type(torch.long)\n\n    if device is None:\n        device = next(model.parameters()).device\n\n    for i, (x_batch, y_batch) in enumerate(loader):\n        model.eval()\n        output = model(x_batch.to(device))\n        features = torch.cat([features, output.detach().cpu()])\n        labels = torch.cat([labels, y_batch.cpu()])\n\n    return TensorDataset(features, labels)\n\n\ndef test_preprocessed_dataset():\n    x = torch.rand(10,3,244,244)\n    y = torch.rand(10,1)\n    ds = TensorDataset(x,y)\n    dl = DataLoader(ds, 16, False)\n    tpp = preprocessed_dataset(alex, dl)\n    assert tpp.tensors[0].shape == torch.Size([10, 4096])\n\n\ntrain_preproc = preprocessed_dataset(alex, train_loader)\nval_preproc = preprocessed_dataset(alex, val_loader)\n\nmake sure that the tensort types are correct:\n\nassert next(iter(train_preproc))[0].type() == 'torch.FloatTensor'\nassert next(iter(train_preproc))[1].type() == 'torch.LongTensor'\n\nwe now build new DataLoaders:\n\nnew_train_loader = DataLoader(train_preproc, 16, True)\nnew_val_loader = DataLoader(val_preproc, 16)\n\n\n\nTop layer\nWith features extracted we can now create a brand new simple model using a fully-connected layer per table suggestions nn.Linear(4096, num_classes):\nLet’s create a new model:\n\ntorch.manual_seed(17)\ntop_layer = nn.Linear(4096, 3)\nmulti_loss_fn = nn.CrossEntropyLoss(reduction='mean')\noptimizer_top = optim.Adam(top_layer.parameters(), lr=3e-4)\nsbs_top = StepByStep(top_layer, optimizer_top, multi_loss_fn)\nsbs_top.set_loaders(new_train_loader, new_val_loader)\n\n\nsbs_top.train(10)\n\n100%|██████████| 10/10 [00:01<00:00,  5.11it/s]\n\n\n\n_ = sbs_top.plot_losses()\n\n\n\n\n\nsbs_top.accuracy\n\n95.97\n\n\n\nsbs_top.accuracy_per_class\n\ntensor([[109, 124],\n        [124, 124],\n        [124, 124]])\n\n\nAnd this is pretty good, and very fast too!\nFor any new images that need to be evaluated thought, we will have to go through the whole model, so let’s insert this new top_layer into a sbs. Important: be very careful when changing the model layers AFTER creating StepByStep object. Model change that changes parameters must be reflected in optimizer as well (I added some check for this via sbs.check_consistency that checks number of parameters).\n\nsbs.model.classifier[6] = top_layer\nsbs.check_consistency()\nsbs.model.classifier\n\nSequential(\n  (0): Dropout(p=0.5, inplace=False)\n  (1): Linear(in_features=9216, out_features=4096, bias=True)\n  (2): ReLU(inplace=True)\n  (3): Dropout(p=0.5, inplace=False)\n  (4): Linear(in_features=4096, out_features=4096, bias=True)\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=4096, out_features=3, bias=True)\n)\n\n\nLet’s evaluate on the old val_loader to see if we get the same result:\n\nsbs.accuracy_per_class\n\ntensor([[109, 124],\n        [124, 124],\n        [124, 124]])\n\n\n\nsbs.accuracy\n\n95.97\n\n\nyup, exactly the same.\n\n\nInception Model\nLet’s try Inception model. Inception has these 2 layers so we can not run the feature extraction, we have to use the full model. First we prep the data:\n\nweights = Inception_V3_Weights.IMAGENET1K_V1\ntransform = weights.transforms()\ntrain_dataset = ImageFolder(root='rps', transform=transform)\nval_dataset = ImageFolder(root='rps-test-set', transform=transform)\ntrain_loader = DataLoader(train_dataset, 16, True)\nval_loader = DataLoader(val_dataset, 16)\n\n\ntransform\n\nImageClassification(\n    crop_size=[299]\n    resize_size=[342]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\n\nnext we set the model and freeze it:\n\ninception = inception_v3(weights=weights)\n\n\ninception\n\nInception3(\n  (Conv2d_1a_3x3): BasicConv2d(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2a_3x3): BasicConv2d(\n    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2b_3x3): BasicConv2d(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv2d_3b_1x1): BasicConv2d(\n    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_4a_3x3): BasicConv2d(\n    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Mixed_5b): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5c): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5d): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6a): InceptionB(\n    (branch3x3): BasicConv2d(\n      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6b): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6c): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6d): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6e): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (AuxLogits): InceptionAux(\n    (conv0): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (conv1): BasicConv2d(\n      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (Mixed_7a): InceptionD(\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2): BasicConv2d(\n      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7b): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7c): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\ninception = inception_v3(weights=weights)\nfreeze_model(inception)\nprint_trainable_parameters(inception)\n\nNo trainable parameters.\n\n\nreplace the top layers per advice:\n\ninception.fc = nn.Linear(2048, 3)\ninception.AuxLogits.fc = nn.Linear(768, 3)\nprint_trainable_parameters(inception)\n\nAuxLogits.fc.weight\nAuxLogits.fc.bias\nfc.weight\nfc.bias\n\n\nWe need to create special loss function that handles 2 output losses (one main and one auxilary) and combines them (with weight of 0.4 for auxilary):\n\ndef inception_loss(outputs, labels):\n    try:\n        main, aux = outputs  # this is a KEY difference from other models with single output\n    except ValueError:\n        main, aux = outputs, None  # this is a typical loss with no auxilairy layers\n        \n    main_loss = nn.CrossEntropyLoss()(main, labels)\n    aux_loss = nn.CrossEntropyLoss()(aux, labels) if aux is not None else 0\n    return main_loss + 0.4 * aux_loss\n\nWe are now ready to create sbs object:\n\ntorch.manual_seed(17)\noptimizer = optim.Adam(inception.parameters(), lr=3e-4)\nsbs_inception = StepByStep(inception, optimizer, inception_loss)\nsbs_inception.set_loaders(train_loader, val_loader)\n\nIt is training time:\n\nsbs_inception.train(10)\n\n100%|██████████| 10/10 [05:08<00:00, 30.85s/it]\n\n\n\nprint(sbs_inception.accuracy_per_class)\nprint(sbs_inception.accuracy)\nsbs_inception.plot_losses()\n\ntensor([[112, 124],\n        [118, 124],\n        [113, 124]])\n92.2\n\n\n\n\n\n\n\nInception model from scratch\n\nscissors = Image.open('rps/scissors/scissors01-001.png')\nimage = ToTensor()(scissors)[:3, :, :].view(1, 3, 300, 300)\nweights = torch.tensor([0.2126, 0.7152, 0.0722]).view(1, 3, 1, 1)\nconvolved = F.conv2d(input=image, weight=weights)\nconverted = ToPILImage()(convolved[0])\n\ngrayscale = scissors.convert('L')\n\n\ngrayscale\n\n\n\n\nOne must define all layers in the __init__ in order to initialize them. Remeber that forward method will be called during training so no layers with parameters should be defined there:\n\nclass Inception(nn.Module):\n    def __init__(self, in_channels):\n        super(Inception, self).__init__()\n        # in_channels@HxW -> 2@HxW\n        self.branch1x1_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n\n        # in_channels@HxW -> 2@HxW -> 3@HxW\n        self.branch5x5_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n        self.branch5x5_2 = nn.Conv2d(2, 3, kernel_size=5, padding=2)\n\n        # in_channels@HxW -> 2@HxW -> 3@HxW\n        self.branch3x3_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n        self.branch3x3_2 = nn.Conv2d(2, 3, kernel_size=3, padding=1)\n\n        # in_channels@HxW -> in_channels@HxW -> 1@HxW\n        self.branch_pool_1 = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n        self.branch_pool_2 = nn.Conv2d(in_channels, 2, kernel_size=1)\n\n    def forward(self, x):\n        # Produces 2 channels\n        branch1x1 = self.branch1x1_1(x)\n        # Produces 3 channels\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n        # Produces 3 channels\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n        # Produces 2 channels\n        branch_pool = self.branch_pool_1(x)\n        branch_pool = self.branch_pool_2(branch_pool)\n        # Concatenates all channels together (10)\n        outputs = torch.cat([branch1x1, branch5x5, branch3x3, branch_pool], 1)\n        return outputs\n\n\ninception = Inception(in_channels=3)\noutput = inception(image)\noutput.shape\n\ntorch.Size([1, 10, 300, 300])"
  },
  {
    "objectID": "1_binary_image_classification.html",
    "href": "1_binary_image_classification.html",
    "title": "Binary image classification",
    "section": "",
    "text": "Here we’ll take a problem of binary image classifing.\n\n\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom pytorched.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\n\n\n\n\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\n\n\nData\nWe’ll use generated data, where images with horizontal and vertical lines are considered label 0, while diagonal have label 1.\n\n\nCode\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start > 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets > 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30):\n    n_rows = n_plot // 6 + ((n_plot % 6) > 0)\n    fig, axes = plt.subplots(n_rows, 6, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // 6, i % 6    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 12})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n\n\nimages, labels = generate_dataset(img_size=5, n_images=300, binary=True, seed=13)\n\n\nfig = plot_images(images, labels, n_plot=30)\n\n\n\n\n\n\nData preparation\n\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels.reshape(-1, 1)).float()  # reshaped this to (N,1) tensor\n\nPyTorch has Dataset class, TensorDataset as a subclass, and we can create custom subclasses too that can handle data augmentation:\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\n\nA torch.utils.data.random_split method can split indices into train and valid (it requires exact number of images to split):\n\ntorch.manual_seed(13)\nN = len(x_tensor)\nn_train = int(.8*N)\nn_val = N - n_train\ntrain_subset, val_subset = random_split(x_tensor, [n_train, n_val])\ntrain_subset\n\n<torch.utils.data.dataset.Subset>\n\n\nwe just need indices:\n\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\n\ntrain_idx[:10]\n\n[118, 170, 148, 239, 226, 146, 168, 195, 6, 180]\n\n\n\n\nData augmentation\nFor data augmentation we only augment training data, so we create training and validation Composer:\n\ntrain_composer = Compose([RandomHorizontalFlip(p=.5),\n                          Normalize(mean=(.5,), std=(.5,))])\n\nval_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n\nNow we can build train/val tensors, Datasets and DataLoaders:\n\nx_train_tensor = x_tensor[train_idx]\ny_train_tensor = y_tensor[train_idx]\n\nx_val_tensor = x_tensor[val_idx]\ny_val_tensor = y_tensor[val_idx]\n\ntrain_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\nval_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n\nWe could stop here and just make loaders:\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\nor we can even used WeightedRandomSampler if we want to balance datasets:\n\ndef make_balanced_sampler(y):\n    # Computes weights for compensating imbalanced classes\n    classes, counts = y.unique(return_counts=True)\n    weights = 1.0 / counts.float()\n    sample_weights = weights[y.squeeze().long()]\n    # Builds sampler with compute weights\n    generator = torch.Generator()\n    sampler = WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        generator=generator,\n        replacement=True\n    )\n    return sampler\n\nNote that we don’t need a val_sampler anymore since we already split datasets:\n\ntrain_sampler = make_balanced_sampler(y_train_tensor)\n\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_sampler)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\n\n\nLogistic Regression Model\n\nlr = 0.1\n\n# Now we can create a model\nmodel_logistic = nn.Sequential()\nmodel_logistic.add_module('flatten', nn.Flatten())\nmodel_logistic.add_module('output', nn.Linear(25, 1, bias=True))\nmodel_logistic.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_logistic = optim.SGD(model_logistic.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\n\n\nsbs_logistic = StepByStep(model_logistic, optimizer_logistic, binary_loss_fn)\nsbs_logistic.set_seed()\nsbs_logistic.set_loaders(train_loader, val_loader)\nsbs_logistic.train(200)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06<00:00, 29.36it/s]\n\n\n\nfig = sbs_logistic.plot_losses()\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_logistic.loader_apply(sbs_logistic.val_loader, sbs_logistic.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [34, 36]])\n\n\nAfter 200 epoch it’s almost 100%. Let’s add 400 more:\n\nsbs_logistic.train(400)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:13<00:00, 30.56it/s]\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_logistic.loader_apply(sbs_logistic.val_loader, sbs_logistic.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [36, 36]])\n\n\nso after 600 epoch model is 100% accurate (at least on 60 samples).\n\n\nDeeper Model\n\nlr = 0.1\n\n# Now we can create a model\nmodel_deeper = nn.Sequential()\nmodel_deeper.add_module('flatten', nn.Flatten())\nmodel_deeper.add_module('linear1', nn.Linear(25, 10, bias=True))\nmodel_deeper.add_module('relu', nn.ReLU())\nmodel_deeper.add_module('linear2', nn.Linear(10, 1, bias=True))\nmodel_deeper.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_deeper = optim.SGD(model_deeper.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\n\n\nsbs_deeper = StepByStep(model_deeper, optimizer_deeper, binary_loss_fn)\nsbs_deeper.set_seed()\nsbs_deeper.set_loaders(train_loader, val_loader)\nsbs_deeper.train(20)\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 28.33it/s]\n\n\n\nfig = sbs_deeper.plot_losses()\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_deeper.loader_apply(sbs_deeper.val_loader, sbs_deeper.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [36, 36]])\n\n\nand even after 20 epoch it’s 100% accurate. We can train more to flatten the loss though which will surely generalize model:\n\nsbs_deeper.train(200)\nfig = sbs_deeper.plot_losses()\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06<00:00, 30.26it/s]\n\n\n\n\n\nAnd that’s it."
  },
  {
    "objectID": "rock_paper_scissors.html",
    "href": "rock_paper_scissors.html",
    "title": "Rock paper scissors",
    "section": "",
    "text": "We’ll use Rock, Paper, Scissors dataset created by Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com) and can be found in his site: Rock Paper Scissors Dataset."
  },
  {
    "objectID": "rock_paper_scissors.html#temporary-dataset",
    "href": "rock_paper_scissors.html#temporary-dataset",
    "title": "Rock paper scissors",
    "section": "Temporary dataset",
    "text": "Temporary dataset\nWe need to calculate normalization parameters (mean and std) for all training images first. This is important step as we will use these normalization parameters not only for training images but for all the validation, and any future, predictions. Since we need to only calculate normalization parameters, we can also scale images to smaller size, just so the calculation is faster.\n\ncomposer = Compose([Resize(28),\n                    ToTensor()])\ntemp_dataset = ImageFolder(root='rps', transform=composer)\ntemp_loader = DataLoader(temp_dataset, batch_size=32)\nnormalizer = StepByStep.make_normalizer(temp_loader)\nnormalizer"
  },
  {
    "objectID": "rock_paper_scissors.html#real-dataset",
    "href": "rock_paper_scissors.html#real-dataset",
    "title": "Rock paper scissors",
    "section": "Real dataset",
    "text": "Real dataset\n\ncomposer = Compose([Resize(28), ToTensor(), normalizer])\ntrain_dataset = ImageFolder(root='rps', transform=composer)\nval_dataset = ImageFolder(root='rps-test-set', transform=composer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n\nimages, labels = next(iter(train_loader))\n\n\ntrain_loader\n\n<torch.utils.data.dataloader.DataLoader>\n\n\n\nlabels\n\ntensor([2, 0, 2, 0, 0, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0, 1])"
  },
  {
    "objectID": "linear_regression.html",
    "href": "linear_regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "Inspiration by Daniel Voigt Godoy’s books\n\nimport platform\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.linear_model import LinearRegression\nfrom pytorched.step_by_step import StepByStep\n\nfrom torchviz import make_dot\nplt.style.use('fivethirtyeight')\n\n\nGenerate some data\nwe’ll use numpy for this, and also need to split the data, can also use numpy for this\n\nnp.random.seed(43)\n\nb_true = 2.\nw_true = -0.5\nN = 100\n\nx = np.random.rand(N,1)\nepsilon = 0.05 * np.random.randn(N,1)\ny = w_true*x + b_true + epsilon\n\nplt.plot(x,y,'.')\nplt.show()\n\n\n\n\n\n\nLinear regression with sklearn\nOf course we can make a fit using sklearn:\n\nreg = LinearRegression().fit(x, y)\nr2_coef = reg.score(x, y)\nprint(reg.coef_, reg.intercept_, r2_coef)\n\n[[-0.52894853]] [2.01635764] 0.9014715901595961\n\n\nbut the point is to learn PyTorch and solve much bigger problems.\n\n\nCreate datasets, data loaders\n\ndata set is the object that holds features and labels together,\nsplit the data into train and valid,\nconvert to pytorch tensors,\ncreate datasets,\ncreate data_loaders.\n\n\nnp.random.seed(43)\nindices = np.arange(N)\nnp.random.shuffle(indices)\ntrain_indices = indices[:int(0.8*N)]\nval_indices = indices[int(0.8*N):]\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrain_x = torch.tensor(x[train_indices], dtype=torch.float32, device=device)\ntrain_y = torch.tensor(y[train_indices], dtype=torch.float32, device=device)\nval_x = torch.tensor(x[val_indices], dtype=torch.float32, device=device)\nval_y = torch.tensor(y[val_indices], dtype=torch.float32, device=device)\n\ntrain_dataset = TensorDataset(train_x, train_y)\nval_dataset = TensorDataset(val_x, val_y)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n\n\nModel, loss, and optimizer\n\ntorch.random.manual_seed(42)\nmodel = torch.nn.Linear(1,1, bias=True, device=device)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nloss_fn = nn.MSELoss()\n\n\n\nTrain\n\nmodel.reset_parameters()\nsbs = StepByStep(model, optimizer, loss_fn)\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(30)\n\n\nsbs.model.state_dict()\n\nOrderedDict([('weight', tensor([[-0.5267]])), ('bias', tensor([2.0177]))])\n\n\n\nsbs.plot_losses()\n\n\n\n\nNote btw that alex and sbs.model are the same object:\n\nassert id(sbs.model) == id(model)\n\n\n\nPredict\n\ntest = np.random.rand(100,1)\ntest_predictions = sbs.predict(test)\nplt.plot(x,y,'.')\nplt.plot(test,test_predictions,'.')\nplt.show()\n\n\n\n\n\n\nSave/load model\n\nsbs.save_checkpoint('pera.pth')\n\n\nsbs.load_checkpoint('pera.pth')\n\n\n\nVisualize model\nOne can use make_dot(yhat) locally. I can’t make graphviz work on GitHub, but the output looks like this:\n\n\n\nSet up tensorboard\nOne can add tensorboard to monitor losses, this will be important when having long training. We can start tensorboard from terminal using tensorboard --logdir runs (or from notebook if using extension via %load_ext tensorboard). The tensorboard should be running at http://localhost:6006/ (ignore \"TensorFlow installation not found\" message, we don’t need it). Make sure path is right, tensorboard will be empty if it can’t find the runs folder."
  },
  {
    "objectID": "binary_classification.html",
    "href": "binary_classification.html",
    "title": "Binary Classification",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom pytorched.step_by_step import StepByStep, RUNS_FOLDER_NAME\nimport platform\n\nimport datetime\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\nWe’ll use Scikit-Learn’s make_moons to generate a toy dataset with 1000 data points and two features."
  },
  {
    "objectID": "binary_classification.html#data-generation",
    "href": "binary_classification.html#data-generation",
    "title": "Binary Classification",
    "section": "Data Generation",
    "text": "Data Generation\n\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=11)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)\n\nWe first use Scikit-Learn’s StandardScaler to standardize datasets:\n\nsc = StandardScaler()\nsc.fit(X_train)  # always fit only on X_train\nX_train_scalled = sc.transform(X_train)\nX_val_scalled = sc.transform(X_val)  # DO NOT use fit or fit_transform on X_val, it causes data leak\nm = sc.mean_\nv = sc.var_\nprint(m, v)\nassert ((X_train_scalled[0] - m)/np.sqrt(v) - X_train[0] < np.finfo(float).eps).all()\n\n[0.4866699  0.26184213] [0.80645937 0.32738853]\n\n\n\nX_train = X_train_scalled\nX_val = X_val_scalled\n\n\nfrom matplotlib.colors import ListedColormap\n\ndef figure1(X_train, y_train, X_val, y_val, cm_bright=None):\n    if cm_bright is None:\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)#, edgecolors='k')\n    ax[0].set_xlabel(r'$X_1$')\n    ax[0].set_ylabel(r'$X_2$')\n    ax[0].set_xlim([-2.3, 2.3])\n    ax[0].set_ylim([-2.3, 2.3])\n    ax[0].set_title('Generated Data - Train')\n\n    ax[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright)#, edgecolors='k')\n    ax[1].set_xlabel(r'$X_1$')\n    ax[1].set_ylabel(r'$X_2$')\n    ax[1].set_xlim([-2.3, 2.3])\n    ax[1].set_ylim([-2.3, 2.3])\n    ax[1].set_title('Generated Data - Validation')\n    fig.tight_layout()\n    \n    return fig\n\n\nfig = figure1(X_train, y_train, X_val, y_val)"
  },
  {
    "objectID": "binary_classification.html#data-preparation",
    "href": "binary_classification.html#data-preparation",
    "title": "Binary Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe preparation of data starts by converting the data points from Numpy arrays to PyTorch tensors and sending them to the available device:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Builds tensors from numpy arrays\nx_train_tensor = torch.as_tensor(X_train).float()\ny_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()  # reshape makes shape from (80,) to (80,1)\n\nx_val_tensor = torch.as_tensor(X_val).float()\ny_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n\n\ntrain_data = TensorDataset(x_train_tensor, y_train_tensor)\nval_data = TensorDataset(x_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_data, 64, shuffle=True)\nval_loader = DataLoader(val_data, 64)"
  },
  {
    "objectID": "binary_classification.html#linear-model",
    "href": "binary_classification.html#linear-model",
    "title": "Binary Classification",
    "section": "Linear model",
    "text": "Linear model\n\ntorch.manual_seed(42)\n\nlr = 0.01\n\nmodel = nn.Sequential()\nmodel.add_module('linear', nn.Linear(2,1))\nmodel.add_module('sigmoid', nn.Sigmoid())\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\nloss_fn = nn.BCELoss()\n\n\nsbs_lin = StepByStep(model, optimizer, loss_fn)\nsbs_lin.set_loaders(train_loader, val_loader)\nsbs_lin.train(100)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 42.69it/s]\n\n\n\n_ = sbs_lin.plot_losses()\n\n\n\n\nLet’s predict the values for X_train (y_train_predicted) and X_val (y_val_predicted) and plot them. Also let’s see how good of a job linear regression did using confusion matrix:\n\ndef predict_plot_count(sbs):\n    y_train_predicted = sbs.predict(X_train)\n    y_val_predicted = sbs.predict(X_val)\n    fig = figure1(X_train, y_train_predicted, X_val, y_val_predicted)\n    print('Confusion matrix:')\n    print(confusion_matrix(y_val, list(map(int, (y_val_predicted > 0.5).ravel()))))\n    print('Correct categories:')\n    print(sbs.loader_apply(sbs.val_loader, sbs.correct))\n\n\npredict_plot_count(sbs_lin)\n\nConfusion matrix:\n[[82 14]\n [14 90]]\nCorrect categories:\ntensor([[ 82,  96],\n        [ 90, 104]])\n\n\n\n\n\nand we see there are some false positives and false negatives (off-diagonal elements)."
  },
  {
    "objectID": "binary_classification.html#two-layer-model",
    "href": "binary_classification.html#two-layer-model",
    "title": "Binary Classification",
    "section": "Two-layer model",
    "text": "Two-layer model\nLet’s make a better model.\n\nmodel_nonlin = nn.Sequential()\nmodel_nonlin.add_module('linear1', nn.Linear(2,10))\nmodel_nonlin.add_module('relu', nn.ReLU())\nmodel_nonlin.add_module('linear2', nn.Linear(10,1))\nmodel_nonlin.add_module('sigmoid', nn.Sigmoid())\noptimizer = optim.Adam(model_nonlin.parameters(), lr=lr)\n\nsbs_nonlin = StepByStep(model_nonlin, optimizer, loss_fn)\nsbs_nonlin.set_loaders(train_loader, val_loader)\nsbs_nonlin.train(100)\n_ = sbs_nonlin.plot_losses()\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.74it/s]\n\n\n\n\n\n\npredict_plot_count(sbs_nonlin)\n\nConfusion matrix:\n[[86 10]\n [10 94]]\nCorrect categories:\ntensor([[ 86,  96],\n        [ 94, 104]])\n\n\n\n\n\nAnd this is better (we could calculate precision and recall)."
  }
]
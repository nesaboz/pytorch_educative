{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OEcqpVrez6BF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpXck5F4z6BF"
   },
   "source": [
    "# Chapter 4: Rethinking the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOkBYfA_z6BG"
   },
   "source": [
    "Now that you've dived a little bit deeper into PyTorch's Datasets and DataLoaders, it's time to put your knowledge into action :-)\n",
    "\n",
    "We're using the same synthetic dataset from the previous challenges (*b = 0.5* and *w = -3* for a **linear regression with a single feature (x)**), but this time you'll be implementing mini-batch gradient descent in PyTorch.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "y = b + w x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmvCo0mFz6BH"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fq9Ezbicz6BH"
   },
   "outputs": [],
   "source": [
    "true_b = .5\n",
    "true_w = -3\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEgnNQq2z6BH"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P95MHr5mz6BI"
   },
   "source": [
    "The preparation of data starts by **converting the data points** from Numpy arrays to PyTorch tensors and sending them to the available **device**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nvgId-8wz6BI"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them \n",
    "# into PyTorch's Tensors and then we send them to the \n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).float().to(device)\n",
    "y_val_tensor = torch.as_tensor(y_val).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5aDsLlz6BJ"
   },
   "source": [
    "But, this time, the data preparation also includes creating datasets and data loaders for both training and validation sets. That's your task now - you're free to choose any mini-batch size you want (and we encourage you to play with different values), but we suggest you to start with 16:\n",
    "\n",
    "Hint: you can use a simple `TensorDataset` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r3gp9fDtz6BJ"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, 32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLVByGUCz6BJ"
   },
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxmtLMPTz6BK"
   },
   "source": [
    "The model configuration not only includes the definition of model, optimizer, and loss function, but also the creation of functions to perform both **training and validation steps**. You can use the **helper methods** below for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GlE0qMdTz6BK"
   },
   "outputs": [],
   "source": [
    "def make_train_step_fn(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step_fn(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "        loss.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return perform_train_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "trgfFQ1rz6BK"
   },
   "outputs": [],
   "source": [
    "def make_val_step_fn(model, loss_fn):\n",
    "    # Builds function that performs a step in the validation loop\n",
    "    def perform_val_step_fn(x, y):\n",
    "        # Sets model to EVAL mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "        return loss.item()\n",
    "    \n",
    "    return perform_val_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skoeeKONz6BL"
   },
   "source": [
    "Your task is, once again, to define a **model**, an **optimizer**, and a **loss function** to tackle our **linear** regression with a **single input** and **single output**. Then, you should use these elements (and the helper methods above) to create your `train_step_fn` and `val_step_fn` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FE2Qw9Pxz6BL"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1,1))\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "val_step_fn = make_val_step_fn(model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5X-CsZJz6BM"
   },
   "source": [
    "## Mini-Batch Inner Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXpQcwo8z6BM"
   },
   "source": [
    "Your task is to implement a function that **executes the mini-batch inner loop**. Given a *data loader*, a device, and a **step function** (that could be either `train_step_fn` or `val_step_fn`), the function should:\n",
    "\n",
    "- loop over the mini-batches yielded by the data loader\n",
    "- send the mini-batch data (x and y) to the device\n",
    "- execute the `step_fn` using x and y\n",
    "- appends the returned loss to the list of `mini_batch_losses`\n",
    "\n",
    "In the end, the `mini_batch` function will return the **average loss over all mini-batches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hDFWcBJkz6BM"
   },
   "outputs": [],
   "source": [
    "def mini_batch(device, data_loader, step_fn):\n",
    "    mini_batch_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        mini_batch_losses.append(step_fn(x_batch, y_batch))\n",
    "\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe2xCbK5z6BN"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eer3e8g_z6BN"
   },
   "source": [
    "Your task is to implement mini-batch gradien descent using the `mini_batch` function you've just implemented above to execute both **training** and **validation steps**:\n",
    "\n",
    "Obs.: the parameter update is happening **inside the training step function** now, that's why you only see the losses in the loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZJI4uGI-z6BN"
   },
   "outputs": [],
   "source": [
    "# Defines number of epochs\n",
    "n_epochs = 20\n",
    "\n",
    "losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZJI4uGI-z6BN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-3.0306]])), ('0.bias', tensor([0.5204]))])\n",
      "0.008385638240724802 0.008410155773162842\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(f\"runs/simple_linear_regression_{datetime.now().strftime('%Y-%m-%d_%H_%M_%S')}\")\n",
    "\n",
    "i_epoch = checkpoint['epoch'] if 'checkpoint' in locals() else 0\n",
    "    \n",
    "n_epochs = 200 \n",
    "\n",
    "for epoch in range(i_epoch+1, i_epoch+1+n_epochs):\n",
    "    # inner loop\n",
    "    loss = mini_batch(device, train_loader, train_step_fn)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # VALIDATION\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step_fn)\n",
    "        val_losses.append(val_loss)   \n",
    "        \n",
    "    writer.add_scalars(main_tag='loss',      # 1)\n",
    "                   tag_scalar_dict={\n",
    "                        'training': loss, \n",
    "                        'validation': val_loss},\n",
    "                   global_step=epoch)\n",
    "    \n",
    "    checkpoint = {'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': losses,\n",
    "              'val_loss': val_losses}\n",
    "\n",
    "    torch.save(checkpoint, 'model_checkpoint.pth')\n",
    "    \n",
    "#     # mimic something getting broken after 20 iterations\n",
    "#     if epoch == 20:\n",
    "#         break\n",
    "        \n",
    "print(model.state_dict())\n",
    "print(losses[-1], val_losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30984), started 0:00:28 ago. (Use '!kill 30984' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-997940941011fa74\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-997940941011fa74\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_whD-MCz6BN"
   },
   "source": [
    "## Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUhSVHdYz6BN"
   },
   "source": [
    "Once the model is fully trained, you may **save it to disk**. Your task is to build a dictionary containing all relevant information, and using `torch.save` to save this dictionary to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37rvCCC3z6BO"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': losses,\n",
    "              'val_loss': val_losses}\n",
    "\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwnOyh3kz6BO"
   },
   "source": [
    "## Loading Models and Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2orgHncz6BO"
   },
   "source": [
    "Once your model is saved to disk, you can load it back to either continue training it or deliver predictions (if it's already fully trained). Your first task is to load both the model and the optimizer states from a file and restore them into the `new_model` and `new_optimizer` respectively. Then, you should make predictions for the `new_inputs` tensor (assuming the loaded model was already fully trained)\n",
    "\n",
    "Hint: don't forget to set the proper mode before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DWZtQSnGz6BO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "losses = checkpoint['loss']\n",
    "val_losses = checkpoint['val_loss'] \n",
    "\n",
    "model.state_dict()\n",
    "model.train()  # IMPORTANT\n",
    "print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "j4zD6U18z6BP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5101],\n",
       "        [-5.5407],\n",
       "        [-8.5712]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1.], [2.], [3]], dtype=torch.float, device=device)\n",
    "\n",
    "model.eval()  # IMPORTANT\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CilyNVJXz6BP"
   },
   "source": [
    "Congratulations! You successfully trained a PyTorch model using **mini-batch** gradient descent, saved it to disk, and \"deployed\" it to make predictions!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenges03_question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
